## ЗАДАНИЕ: Автоматическая обработка текста
**Ссылка**, на источник текста
DATA_URL = "http://az.lib.ru/n/nekrasow_n_a/text_1840_pevitza.shtml"
Устанавливаем библиотеки
import warnings
warnings.filterwarnings('ignore')
!pip install -q git+https://github.com/dvolchek/rnnmorph.git
Создаём объект морфологического анализатора `RNNMorph`
from rnnmorph.predictor import RNNMorphPredictor
predictor = RNNMorphPredictor(language="ru")
Скачиваем текст, по которому будет дано задание, с помощью `urllib`
import urllib.request

opener = urllib.request.URLopener({})
resource = opener.open(DATA_URL)
raw_text = resource.read().decode(resource.headers.get_content_charset()) #Текс с html тегами
raw_text[:200]
Как видно, текст содержит html теги, от которых нужно избавиться. Выбрасываем из текста HTML-теги с помощью библиотеки Beatiful soap
from bs4 import BeautifulSoup
soup = BeautifulSoup(raw_text, features="html.parser")

# kill all script and style elements
for script in soup(["script", "style"]):
    script.extract()    # rip it out

# get text
cleaned_text = soup.get_text()
cleaned_text[:200]
С помощью библиотеки [NLTK](https://nltk.org/) разбиваем текст на предложения и токены.
from nltk.tokenize import sent_tokenize, word_tokenize
import nltk
nltk.download('punkt')

tokenized_sentences = [word_tokenize(sentence) for sentence in sent_tokenize(cleaned_text)]
"A total of %d 'sentences'" % len(tokenized_sentences)
## Задание 1
С помощью метода `str.isalpha` из стандартной библиотеки Python модифицируйте нижеследующий код так, чтобы в predictions остались только буквенные токены.
from tqdm import tqdm

# Функция для фильтрации токенов и оставления только буквенных
def filter_alpha(tokens):
    return [token.normal_form for token in tokens if token.normal_form.isalpha()]

# Применение фильтра к предсказаниям
predictions = [[token for token in filter_alpha(sent)] for sent in tqdm(predictor.predict_sentences(sentences=tokenized_sentences), "sentences")]
predictions[-11:-10] # Теперь должны остаться только буквенные токены

len(predictions)
non_uniq_tokens = [word for sentence in predictions for word in sentence]
print(len(non_uniq_tokens)

Для продолжения работы над заданием числа должны быть близки к указанным
## Задание 2

Используя `non_uniq_tokens`, стоп-слова для русского языка из библиотеки nltk (`nltk.corpus.stopwords`) и `nltk.FreqDist`, вычислите, **какую долю среди 100 самых частотных** токенов в произведении занимают токены, **не относящиеся** к стоп словам.

**Например**, если среди 100 самых частотных слов встречается 25 слов, входящих в стоп лист, значит не входят в стоп лист 75 слов, и их доля составит 0.75.

**Не бойтесь использовать документацию NLTK и тьюториалы.**
import nltk
from nltk import FreqDist
from nltk.corpus import stopwords

nltk.download("stopwords")
STOPWORDS = set(stopwords.words("russian"))

# Пример стоп слов
print(stopwords.words("russian")[:5])

# Создадим частотное распределение токенов
freq_dist = FreqDist(non_uniq_tokens)

# Получим 100 самых частотных токенов
most_common_tokens = freq_dist.most_common(50)

# Посчитаем количество токенов, не входящих в список стоп-слов
tokens_not_in_stopwords = [token for token, freq in most_common_tokens if token not in STOPWORDS]

# Вычислим долю таких токенов среди 100 самых частотных
portion_not_in_stopwords = len(tokens_not_in_stopwords) / len(most_common_tokens)

print("Доля токенов, не входящих в стоп-слова, среди 100 самых частотных:", portion_not_in_stopwords)
